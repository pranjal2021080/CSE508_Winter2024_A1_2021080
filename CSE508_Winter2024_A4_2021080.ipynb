{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from gensim.parsing.preprocessing import remove_stopwords, strip_multiple_whitespaces, strip_numeric\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def cleaning(s):\n",
        "    # Tokenize using spaCy for more accurate tokenization\n",
        "    doc = nlp(s)\n",
        "    tokens = [token.text.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
        "\n",
        "    # Lemmatization using NLTK's WordNetLemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Join tokens back into a string\n",
        "    cleaned_text = ' '.join(tokens)\n",
        "    return cleaned_text\n",
        "\n",
        "# Test the cleaning function\n",
        "text = \"This is a sample text containing URLs like http://example.com and non-alphabetic characters 1234.\"\n",
        "cleaned_text = cleaning(text)\n",
        "print(cleaned_text)\n"
      ],
      "metadata": {
        "id": "ugv7AM_kBT0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.device_count())\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "3yY36O7OBTsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Initialize the model\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "hCQLT1YaBlP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_data, test_data = train_test_split(df_sample, test_size=0.25, random_state=42)\n",
        "\n",
        "# Check the shape of the training and testing sets\n",
        "print(\"Training data shape:\", train_data.shape)\n",
        "print(\"Testing data shape:\", test_data.shape)"
      ],
      "metadata": {
        "id": "foxpiLoSBn-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom token for summarization task\n",
        "SUMMARY_TOKEN = \" [TL;DR] \"\n",
        "\n",
        "# Concatenate text, TL;DR token, and summary for training data\n",
        "train_data['Processed_Input'] = train_data['Processed_Text'] + SUMMARY_TOKEN + train_data['Processed_Summary']\n",
        "\n",
        "# Create an array containing processed input for training\n",
        "input_data_train = train_data['Processed_Input'].tolist()\n",
        "print(input_data_train)\n"
      ],
      "metadata": {
        "id": "ZQlJA6AKB0zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['Test_Text'] = test_data['Text'] + \" TL;DR\"\n",
        "reviews_array_test = test_data['Test_Text'].tolist()\n",
        "print(reviews_array_test)"
      ],
      "metadata": {
        "id": "2_KOYE_DB8zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Define a unique token to denote TL;DR\n",
        "TLDR_TOKEN = \"<TLDR>\"\n",
        "\n",
        "class CustomReviewDataset(Dataset):\n",
        "    def __init__(self, reviews, tokenizer, max_len):\n",
        "        self.data = reviews\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        review = self.data[idx]\n",
        "        text, tldr, summary = review['Text'], review['TLDR'], review['Summary']\n",
        "\n",
        "        # Concatenate text, TL;DR, and summary with unique tokens\n",
        "        combined_text = f\"{text} {TLDR_TOKEN} {tldr} {summary}\"\n",
        "\n",
        "        # Tokenize the combined text\n",
        "        tokens = self.tokenizer.encode(combined_text, add_special_tokens=True, max_length=self.max_len, truncation=True)\n",
        "\n",
        "        # Pad tokens to max_len\n",
        "        tokens += [self.tokenizer.pad_token_id] * (self.max_len - len(tokens))\n",
        "\n",
        "        # Prepare labels (shifted by one position) for LM training\n",
        "        labels = tokens[1:] + [self.tokenizer.pad_token_id]\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(tokens),\n",
        "            'labels': torch.tensor(labels)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "_r7g6rjECA8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./training_results',         # output directory\n",
        "    num_train_epochs=3,                      # number of training epochs\n",
        "    per_device_train_batch_size=16,          # batch size per device during training\n",
        "    logging_dir='./training_logs',           # directory for storing logs\n",
        "    logging_steps=100,                       # log every 100 steps\n",
        "    save_steps=100                           # save checkpoint every 100 steps\n",
        ")\n",
        "\n",
        "# Create the DataLoader\n",
        "train_loader = DataLoader(train_data, batch_size=training_args.per_device_train_batch_size, shuffle=True)\n",
        "\n",
        "# Define the Trainer for fine-tuning\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_loader,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Custom training loop\n",
        "for epoch in range(training_args.num_train_epochs):\n",
        "    for batch in train_loader:\n",
        "        # Forward pass\n",
        "        inputs = batch['input_ids']\n",
        "        labels = batch['labels']\n",
        "        outputs = model(inputs, labels=labels)\n",
        "\n",
        "        # Backward pass and update\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Logging\n",
        "        if trainer.is_world_process_zero() and trainer.args.logging_steps and trainer.step % trainer.args.logging_steps == 0:\n",
        "            trainer.log_metrics('train', epoch=epoch)\n",
        "\n",
        "# Save the model\n",
        "trainer.save_model('./fine_tuned_model')\n"
      ],
      "metadata": {
        "id": "IXXIiUm6COjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"./results\"\n",
        "model.save_pretrained(model_path)"
      ],
      "metadata": {
        "id": "Gm5CbpDTCef6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model_path = \"./results\\checkpoint-100\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Assuming you have a ReviewDataset class for the test data\n",
        "test_dataset = ReviewDataset(reviews_array_test, tokenizer, max_length=200)\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for index, row in test_data.iterrows():\n",
        "        input_ids = row['input_ids'].unsqueeze(0)  # Unsqueeze to add batch dimension\n",
        "        generated_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=len(input_ids[0])+30,  # Set the maximum length of the generated text\n",
        "            num_beams=4,     # Set the number of beams for beam search\n",
        "            length_penalty=2.0,  # Set the length penalty for beam search\n",
        "            repetition_penalty=2.0,  # Set the repetition penalty for beam search\n",
        "            pad_token_id=tokenizer.pad_token_id,  # Set the pad token ID\n",
        "            eos_token_id=tokenizer.eos_token_id,  # Set the end-of-sequence token ID\n",
        "            early_stopping=True  # Enable early stopping\n",
        "        )\n",
        "        # Decode the generated text\n",
        "        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        print(\"Generated Text:\", generated_text)\n"
      ],
      "metadata": {
        "id": "1wZIQ1LoChln"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}